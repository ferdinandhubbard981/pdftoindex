import os
import re
import nltk
# Download the required NLTK resource
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer, PorterStemmer
import PyPDF2
import reportlab
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
from collections import defaultdict
import argparse
from reportlab.lib.utils import ImageReader
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.pagesizes import letter


def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)


def stem_and_lemmatize(word):
    lemmatizer = WordNetLemmatizer()
    stemmer = PorterStemmer()
    lemma = lemmatizer.lemmatize(word, get_wordnet_pos(word))
    stem = stemmer.stem(word)
    return stem, lemma


def is_conjugation_or_stem(base_word, other_word):
    base_stem, base_lemma = stem_and_lemmatize(base_word)
    other_stem, other_lemma = stem_and_lemmatize(other_word)

    return base_stem == other_stem or base_lemma == other_lemma


def extract_words(pdf_path, whitelist):
    # Open the PDF file
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)

        # Create a dictionary to store words and their page numbers
        word_pages = defaultdict(set)

        # Go through each page
        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            words = re.findall(r'\b\w+\b', page.extract_text())

            # Store each word and its page number
            for word in words:
                word = word.lower()  # make word lowercase
                for base_word in whitelist:
                    if base_word in word or word == base_word or word.rstrip('es') == base_word or word.rstrip('s') == base_word or is_conjugation_or_stem(base_word, word):
                        word_pages[word].add(page_num + 1)  # add 1 to start pages at 1
                        break

    return word_pages


def create_index_pdf(word_pages, dir_path):
    # Create the index.pdf file
    index_pdf_path = os.path.join(dir_path, "index.pdf")

    doc = SimpleDocTemplate(index_pdf_path, pagesize=letter)
    Story = []
    styles = getSampleStyleSheet()

    # Sort the words
    sorted_words = sorted(word_pages.keys(), key=lambda word: word.lower())

    # Write each word and its pages
    for word in sorted_words:
        pages = sorted(word_pages[word])
        text = f"{word}: {', '.join(map(str, pages))}"
        para = Paragraph(text, styles["BodyText"])
        Story.append(para)
        Story.append(Spacer(1, 12))

    doc.build(Story)

def main(args):
    pdf_path = args.pdf
    whitelist_path = args.whitelist
    dir_path = args.dir

    # Read the whitelist words
    with open(whitelist_path, 'r') as file:
        whitelist = set(line.strip().lower() for line in file)  # make whitelist words lowercase

    # Extract the words and their pages from the PDF
    word_pages = extract_words(pdf_path, whitelist)

    # Create the index.pdf file
    create_index_pdf(word_pages, dir_path)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process a PDF file and create an index PDF.")
    parser.add_argument("--pdf", help="Path to the PDF file")
    parser.add_argument("--whitelist", help="Path to the whitelist file")
    parser.add_argument("--dir", help="Path to the output directory")
    args = parser.parse_args()

    main(args)
